{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6484b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario iTC\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 12.5MB/s]                    \n",
      "2022-08-18 21:20:14 WARNING: Language es package default expects mwt, which has been added\n",
      "2022-08-18 21:20:14 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-08-18 21:20:14 INFO: Use device: cpu\n",
      "2022-08-18 21:20:14 INFO: Loading: tokenize\n",
      "2022-08-18 21:20:14 INFO: Loading: mwt\n",
      "2022-08-18 21:20:14 INFO: Loading: lemma\n",
      "2022-08-18 21:20:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re # para expresiones regulares\n",
    "import emoji #convertir emoticones a texto\n",
    "import nltk #tokenizar y stopwords\n",
    "from nltk.corpus import stopwords#importamos el corpus de stopwords de NLTK\n",
    "import stanza #lematizar texto en espa√±ol\n",
    "import matplotlib.pyplot as plt# presentar las graficas\n",
    "# especificamos los procesadores, el lenguaje y con la condicion de que el texto ya esta tokenizado\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,lemma', tokenize_pretokenized=True) #lematizacion\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31438e52",
   "metadata": {},
   "source": [
    "Se sigue el mismo procedimiento que se realiz√≥ para el preprocesamiento de datos extra√≠dos para el entrenamiento \"Carpeta -> 2.Preprocesamiento\", con excepci√≥n de la limpieza manual para que se pueda usar con otros datos nuevos directamente sin necesidad de realizar otros procedimientos manuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53967ad5",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7da9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos Tweets que son REPLIES\n",
    "def eliminar_tweet_replies(dataset):\n",
    "    dataset['length'] = dataset.reply_to.str.len()\n",
    "    dataset = dataset[dataset.length < 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acaddc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_repetidos(dataset):\n",
    "    dataset.drop_duplicates(subset=['username','tweet'], inplace=True)\n",
    "    \n",
    "def eliminar_atributos_innecesarios(dataset):\n",
    "    dataset.drop(['conversation_id', 'created_at', 'timezone', 'user_id', 'username', 'name', 'place', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src', 'trans_dest', 'language', 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video', 'thumbnail', 'near','length'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92dd7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Palabras que no son tomadas en cuenta en la eliminacion de caracteres repetidos\n",
    "palabras_excluidas=['facebook','mood','boomerang', 'boomer', 'tattoo', 'cool','feeling',\n",
    "                   'descoord','tweet','desee','cree','lee'] \n",
    "#eliminar menciones y hashtags\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r'@[A-Za-z0-9_]+', '', texto) #Remueve menciones\n",
    "    texto = re.sub(r\"[#]+\", '', texto) #Remueve hashtags\n",
    "    #eliminamos caracteres repetidos\n",
    "    res = any(bool(True if palabra in texto else False) for palabra in palabras_excluidas)\n",
    "    if not res: #no hay exclusiones\n",
    "        texto = re.sub(r'(a|e|i|o|u|A|E|I|O|U)\\1+', r'\\1', texto) # vocales repetidas\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79411c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminamos simbolos especiales, numeros y espacios repetidos\n",
    "def limpiar_signos(texto):\n",
    "    texto = re.sub(r'[\\‚Äú\\‚Äù\\¬®‚Äò‚Äô,;.‚Ä¶:¬°!¬°¬∞¬´¬ª„Ää„Äã‚Ä¢¬ø?@#$%&[\\](){}<>~=+\\-‚Äì‚Äî*/|\\\\_^`\"\\']', '', texto) #eliminando simbolos\n",
    "    texto = re.sub('\\d', '', texto) #eliminando numeros presentes en el texto\n",
    "    texto = re.sub(r'(\\s)\\1+', r'\\1', texto)  #elimina espacios extra en el texto\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4b1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion de emoticones a texto en idioma espa√±ol\n",
    "def convertir_emojis(texto):\n",
    "    texto=emoji.demojize(texto, language='es')  # üíî --> :coraz√≥n_roto:\n",
    "    return texto\n",
    "\n",
    "def depurar_emojis(texto):\n",
    "    texto = re.sub(r'([:][A-Za-z_]+[:])\\1+', r'\\1', texto)  #eliminar emojis repetidos\n",
    "    texto = re.sub(r' ?([:][A-Za-z_]+[:]) ?', r' \\1 ', texto) #separar emojis unidos\n",
    "    texto = re.sub(r'(\\s)\\1+', r'\\1', texto)  #eliminar espacios excesivos generados en la separacion de emojis\n",
    "    texto = re.sub(r'[:]', '', texto)  #elim. los \":\" del formato de emoji a texto :cara_triste:-->cara_triste\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381c2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELIMINAR TILDES\n",
    "def quitar_tilde(s):\n",
    "    reemplazo = (   #se reemplazan las vocales con tilde por su equivalente sin tilde\n",
    "        (\"√°\", \"a\"),\n",
    "        (\"√©\", \"e\"),\n",
    "        (\"√≠\", \"i\"),\n",
    "        (\"√≥\", \"o\"),\n",
    "        (\"√∫\", \"u\"),\n",
    "        (\"√º\", \"u\"),\n",
    "    )\n",
    "    for a, b in reemplazo:\n",
    "        s = s.replace(a, b)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2809f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texto_a_minusculas(dataset):\n",
    "    #Se convierten todo el texto de los tweets a min√∫sculas\n",
    "    dataset['tweet_preprocesado'] = dataset['tweet_preprocesado'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5af1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZACION\n",
    "from nltk.tokenize import word_tokenize #divide una oraci√≥n en tokens o palabras\n",
    "def tokenizar(texto):\n",
    "    texto=word_tokenize(texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e07806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORDS\n",
    "stopwords_excluidas={'No', 'Nada', 'Ni', 'me', 'mi'}\n",
    "\n",
    "# se obtienen la lista de stopwords en espa√±ol que tiene NLTK\n",
    "stop_words = set(stopwords.words('spanish'))  #recupera las palabras vac√≠as en espa√±ol\n",
    "stop_words.update((\"q\",\"xq\",\"pq\", \"porq\",\"x\",\"d\",\"i\",\"pa\")) #agregamos nuevas stopwords comunmente usadas\n",
    "#print(sorted(stop_words))\n",
    "def filtrar_stopwords(texto):\n",
    "    #quitamos las palabras que estan excluidas de la lista de stopwords\n",
    "    for x in stopwords_excluidas:\n",
    "        if x.lower() in stop_words:\n",
    "            stop_words.remove(x.lower())\n",
    "    #Quitamos tildes a las Stopwords (al ser en espa√±ol)\n",
    "    stop_words_tilde=[]\n",
    "    for x in stop_words:\n",
    "        x=quitar_tilde(x) #usamos el metodo creado anteriormente para quitar las tildes \n",
    "        stop_words_tilde.append(x)\n",
    "    #se toma el texto tokenizado usando el metodo anteriormente creado  para tokenizar\n",
    "    word_tokens = tokenizar(texto) \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens: # se filtran las stopwords\n",
    "        if w not in stop_words_tilde:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711f458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematizacion\n",
    "def lematizar_texto(texto):\n",
    "    doc = nlp([texto])\n",
    "    textLema = []\n",
    "    for sent in doc.sentences: \n",
    "         for word in sent.words:\n",
    "            textLema.append(quitar_tilde(word.lemma)) # texto aplicado la lematizacion -quitamos tildes\n",
    "    return textLema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd152a",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los Tweets del 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b35efe",
   "metadata": {},
   "source": [
    "Importamos dataset extra√≠do de los tweets del 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1c8926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_2019_df = pd.DataFrame(pd.read_excel(\"Tweets_2019.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be2311",
   "metadata": {},
   "source": [
    "Aplicamos todas las funciones de procesamiento a los tweets de forma directa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233f9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos todas las funciones anteriores en una sola\n",
    "def limpiar_tweets(dataset):\n",
    "    eliminar_tweet_replies(dataset)\n",
    "    eliminar_repetidos(dataset)\n",
    "    eliminar_atributos_innecesarios(dataset)\n",
    "    dataset['tweet_preprocesado'] = dataset['tweet'].apply(limpiar_texto)\n",
    "    dataset['tweet_preprocesado'] = dataset['tweet_preprocesado'].apply(limpiar_signos)\n",
    "    dataset['tweet_preprocesado'] = dataset['tweet_preprocesado'].apply(convertir_emojis)\n",
    "    dataset['tweet_preprocesado'] = dataset['tweet_preprocesado'].apply(depurar_emojis)\n",
    "    dataset['tweet_preprocesado'] = dataset['tweet_preprocesado'].apply(quitar_tilde)\n",
    "    texto_a_minusculas(dataset)\n",
    "    dataset['tweet_preprocesado'] = dataset['tweet_preprocesado'].apply(filtrar_stopwords) #tokenizar y filtrar stopwords\n",
    "    dataset['tweet_preprocesado'] = dataset['tweet_preprocesado'].apply(lematizar_texto) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06665f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "limpiar_tweets(tweets_2019_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46fab85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos dataset preprocesado\n",
    "tweets_2019_df.to_excel(\"Dataset_preprocesado_2019.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd7ba4",
   "metadata": {},
   "source": [
    "### Extraer caracter√≠sticas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f095be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el path de la carpeta principal para llamar a archivos guardados\n",
    "import os\n",
    "path = os.path.normpath(os.getcwd() + os.sep + os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19052d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el dataset preprocesado para la extraccion de caracteristicas\n",
    "tweets_2019_df = pd.DataFrame(pd.read_excel(\"Dataset_preprocesado_2019.xlsx\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28cf7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Creamos un nuevo vector tfidf con el antiguo vocabulario guardado de los unigramas (usado en el mejor modelo)\n",
    "transformer = TfidfTransformer()\n",
    "#cargamos el vocabulario\n",
    "loaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(path+\"/3.Extraccion_caracteristicas/vocabulary_Unigrama.pkl\", \"rb\")))\n",
    "#transformamos los nuevos datos\n",
    "tfidf = transformer.fit_transform(loaded_vec.fit_transform(tweets_2019_df[\"tweet_preprocesado\"]))\n",
    "vector_tfdf=tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce1a17",
   "metadata": {},
   "source": [
    "\"tfidf\" tendr√° la misma longitud de caracter√≠sticas que los datos entrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7cfe7",
   "metadata": {},
   "source": [
    "## Predicci√≥n del  dataset mediante el modelo guardado de RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eae8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el modelo que ofreci√≥ mejor rendimiento\n",
    "import joblib \n",
    "modelo_rf = joblib.load(path+'/modelo_RF_Unigram.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e4abdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos el sentimiento\n",
    "sentiment = modelo_rf.predict(vector_tfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edbfbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el resultado de la predicci√≥n en un dataframe (columna \"Sentimiento_Predicho\")\n",
    "prediccion_df=pd.DataFrame()\n",
    "prediccion_df['Sentimiento_Predicho']=pd.DataFrame(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb7baa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>geo</th>\n",
       "      <th>tweet_preprocesado</th>\n",
       "      <th>Sentimiento_Predicho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1.201163e+18</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>10:36:58</td>\n",
       "      <td>Un arquero de ansiedad nada normal.</td>\n",
       "      <td>-2.6189724258280527,-79.5099020664484,28.20628...</td>\n",
       "      <td>['arquero', 'ansiedad', 'nada', 'normal']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>1.080955e+18</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>17:31:28</td>\n",
       "      <td>100 de cada 1 persona que conozco sufre de dep...</td>\n",
       "      <td>-2.6189724258280527,-79.5099020664484,28.20628...</td>\n",
       "      <td>['cada', 'persona', 'conocer', 'sufrir', 'depr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>1.181384e+18</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>20:39:46</td>\n",
       "      <td>Fiel a las pr√°cticas del neoliberalismo ahora ...</td>\n",
       "      <td>-0.1983403079450697,-77.53522051504922,27.8055...</td>\n",
       "      <td>['fiel', 'practica', 'neoliberalismo', 'ahora'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>1.183525e+18</td>\n",
       "      <td>2019-10-13</td>\n",
       "      <td>18:29:25</td>\n",
       "      <td>@HarryPutter_1 Y creo q tu eres parte o pagado...</td>\n",
       "      <td>-2.2579465031735535,-79.76289615008257,28.1849...</td>\n",
       "      <td>['creer', 'parte', 'pagar', 'desesperado', 'de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>1.196482e+18</td>\n",
       "      <td>2019-11-18</td>\n",
       "      <td>12:36:10</td>\n",
       "      <td>Que desmotivada estoy en la u se√±ores.</td>\n",
       "      <td>-2.6189724258280527,-79.5099020664484,28.20628...</td>\n",
       "      <td>['desmotivada', 'u', 'se√±or']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>1.121061e+18</td>\n",
       "      <td>2019-04-24</td>\n",
       "      <td>09:37:16</td>\n",
       "      <td>@elcomerciocom La violencia familiar o la pres...</td>\n",
       "      <td>-0.1983403079450697,-77.53522051504922,27.8055...</td>\n",
       "      <td>['violencia', 'familiar', 'presion', 'economic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id        date      time  \\\n",
       "496   1.201163e+18  2019-12-01  10:36:58   \n",
       "1038  1.080955e+18  2019-01-03  17:31:28   \n",
       "2003  1.181384e+18  2019-10-07  20:39:46   \n",
       "1461  1.183525e+18  2019-10-13  18:29:25   \n",
       "1251  1.196482e+18  2019-11-18  12:36:10   \n",
       "1901  1.121061e+18  2019-04-24  09:37:16   \n",
       "\n",
       "                                                  tweet  \\\n",
       "496                 Un arquero de ansiedad nada normal.   \n",
       "1038  100 de cada 1 persona que conozco sufre de dep...   \n",
       "2003  Fiel a las pr√°cticas del neoliberalismo ahora ...   \n",
       "1461  @HarryPutter_1 Y creo q tu eres parte o pagado...   \n",
       "1251             Que desmotivada estoy en la u se√±ores.   \n",
       "1901  @elcomerciocom La violencia familiar o la pres...   \n",
       "\n",
       "                                                    geo  \\\n",
       "496   -2.6189724258280527,-79.5099020664484,28.20628...   \n",
       "1038  -2.6189724258280527,-79.5099020664484,28.20628...   \n",
       "2003  -0.1983403079450697,-77.53522051504922,27.8055...   \n",
       "1461  -2.2579465031735535,-79.76289615008257,28.1849...   \n",
       "1251  -2.6189724258280527,-79.5099020664484,28.20628...   \n",
       "1901  -0.1983403079450697,-77.53522051504922,27.8055...   \n",
       "\n",
       "                                     tweet_preprocesado  Sentimiento_Predicho  \n",
       "496           ['arquero', 'ansiedad', 'nada', 'normal']                     1  \n",
       "1038  ['cada', 'persona', 'conocer', 'sufrir', 'depr...                     0  \n",
       "2003  ['fiel', 'practica', 'neoliberalismo', 'ahora'...                     0  \n",
       "1461  ['creer', 'parte', 'pagar', 'desesperado', 'de...                     0  \n",
       "1251                      ['desmotivada', 'u', 'se√±or']                     1  \n",
       "1901  ['violencia', 'familiar', 'presion', 'economic...                     0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unimos el dataset cargado con la columna de \"sentimiento_predicho\" para \n",
    "#    comprobar los tweets que fueron clasificados con cada clase (1 y 0)\n",
    "depresion_2019_df = pd.concat([tweets_2019_df, prediccion_df], axis=1) #pd.concat([df1, s1], axis=1)\n",
    "\n",
    "depresion_2019_df.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90a3e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminamos los tweets que fueron clasificados como no depresivos (etiqueta 0)\n",
    "df_filtrado = depresion2019_df[depresion2019_df['Sentimiento_Predicho'] == 1]\n",
    "#Guardamos el dataset solo con los tweets clasificados como depresivos para uso posterior\n",
    "df_filtrado.to_excel(\"Tweets_Depresivos_2019_Predichos.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
